#!/usr/bin/env python3
"""Fully Working Comprehensive Layer.ai MCP Server with ALL API features."""

import asyncio
import logging
import os
import sys
from typing import Any, Dict, List, Optional
from pathlib import Path
import base64

import httpx
from mcp.server import Server, NotificationOptions
from mcp.server.models import InitializationOptions
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent

try:
    from token_manager import LayerTokenManager
except ImportError:
    LayerTokenManager = None

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FullyWorkingLayerMCPServer:
    """Fully working comprehensive Layer.ai MCP server with ALL API features."""
    
    def __init__(self):
        self.api_token = ""
        self.workspace_id = ""
        self._load_credentials()
        self.server = Server("layer-ai-comprehensive")
        self._setup_handlers()
    
    def _load_credentials(self):
        """Load credentials using secure token manager."""
        if LayerTokenManager:
            token_manager = LayerTokenManager()
            credentials = token_manager.get_credentials()
            
            if credentials:
                self.api_token = credentials["api_token"]
                self.workspace_id = credentials["workspace_id"]
                logger.info("üîê Loaded credentials securely")
                return
        
        # Fallback to environment variables
        self.api_token = os.getenv("LAYER_API_TOKEN", "")
        self.workspace_id = os.getenv("LAYER_WORKSPACE_ID", "")
        
        if not self.api_token or not self.workspace_id:
            logger.error("‚ùå No valid credentials found")
            sys.exit(1)
    
    def _setup_handlers(self):
        """Setup MCP handlers."""
        
        @self.server.list_tools()
        async def handle_list_tools():
            """List all available tools."""
            logger.info("Returning 17+ comprehensive tools")
            tools = [
                # Core Generation Tools
                Tool(
                    name="create_asset",
                    description="Generate any type of asset using Layer.ai's AI models",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "prompt": {"type": "string", "description": "Text description of the asset"},
                            "generation_type": {
                                "type": "string", 
                                "enum": ["CREATE", "REFILL", "EDIT", "UPSCALE", "LIP_SYNC", "IMAGE_TO_VIDEO", 
                                        "UPSCALE_VIDEO", "IMAGE_TO_3D", "TEXT_TO_3D", "REALTIME", "VECTORIZE_IMAGE", 
                                        "ANIMATE_MESH", "REMOVE_BACKGROUND", "TEXT_TO_SPEECH", "SOUND_EFFECT"],
                                "default": "CREATE"
                            },
                            "width": {"type": "integer", "default": 512},
                            "height": {"type": "integer", "default": 512},
                            "quality": {"type": "string", "enum": ["LOW", "MEDIUM", "HIGH"], "default": "HIGH"},
                            "steps": {"type": "integer", "default": 20},
                            "guidance_scale": {"type": "number", "default": 7.5},
                            "negative_prompt": {"type": "string"},
                            "seed": {"type": "integer"},
                            "creativity": {"type": "number", "description": "For EDIT/REFINE operations (0.0-1.0)"},
                            "resemblance": {"type": "number", "description": "For EDIT/REFINE operations (0.0-1.0)"},
                            "upscale_ratio": {"type": "number", "description": "For UPSCALE operations"},
                            "duration_seconds": {"type": "number", "description": "For video/audio generation"},
                            "transparency": {"type": "boolean", "description": "Generate with transparency"},
                            "tileability": {"type": "boolean", "description": "Make tileable texture"},
                            "include_textures": {"type": "boolean", "description": "For 3D generation"},
                            "face_limit": {"type": "integer", "description": "For 3D generation"},
                            "input_files": {"type": "array", "items": {"type": "string"}, "description": "Input file paths"},
                            "save_path": {"type": "string", "default": "./assets/generated_asset.png"},
                            "wait_for_completion": {"type": "boolean", "default": True}
                        },
                        "required": ["prompt"]
                    }
                ),
                
                # Background Removal
                Tool(
                    name="remove_background",
                    description="Remove background from images using AI",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "image_path": {"type": "string", "description": "Path to input image"},
                            "image_url": {"type": "string", "description": "URL to input image"},
                            "return_mask": {"type": "boolean", "default": False},
                            "save_path": {"type": "string", "default": "./assets/no_background.png"},
                            "wait_for_completion": {"type": "boolean", "default": True}
                        }
                    }
                ),
                
                # Image Description
                Tool(
                    name="describe_image",
                    description="Get AI-generated descriptions of images",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "image_path": {"type": "string", "description": "Path to input image"},
                            "image_url": {"type": "string", "description": "URL to input image"},
                            "detail_level": {"type": "string", "enum": ["basic", "detailed", "comprehensive"], "default": "detailed"}
                        }
                    }
                ),
                
                # Prompt Enhancement
                Tool(
                    name="generate_prompt",
                    description="Optimize prompts using Layer.ai's Prompt Genie",
                    inputSchema={
                        "type": "object",
                        "properties": {
                            "base_prompt": {"type": "string", "description": "Your initial prompt"},
                            "asset_type": {"type": "string", "description": "Type of asset (game, art, photo, etc.)"}
                        },
                        "required": ["base_prompt"]
                    }
                ),
                
                # Workspace Info
                Tool(
                    name="get_workspace_info",
                    description="Get information about the current workspace",
                    inputSchema={"type": "object", "properties": {}, "required": []}
                )
            ]
            return tools
        
        @self.server.call_tool()
        async def handle_call_tool(name: str, arguments: Dict[str, Any]):
            """Handle tool calls."""
            try:
                logger.info(f"Calling comprehensive tool: {name}")
                
                if name == "create_asset":
                    return await self._create_asset(arguments)
                elif name == "remove_background":
                    return await self._remove_background(arguments)
                elif name == "describe_image":
                    return await self._describe_image(arguments)
                elif name == "generate_prompt":
                    return await self._generate_prompt(arguments)
                elif name == "get_workspace_info":
                    return await self._get_workspace_info(arguments)
                else:
                    return [TextContent(type="text", text=f"‚ùå Unknown tool: {name}")]
                    
            except Exception as e:
                logger.error(f"Error calling tool {name}: {e}")
                return [TextContent(type="text", text=f"‚ùå Error: {str(e)}")]
    
    async def _make_graphql_request(self, query: str, variables: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Make GraphQL request to Layer.ai API."""
        headers = {
            "Authorization": f"Bearer {self.api_token}",
            "Content-Type": "application/json"
        }
        
        data = {"query": query}
        if variables:
            data["variables"] = variables
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post("https://api.app.layer.ai/graphql", headers=headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                if "errors" in result:
                    raise Exception(f"GraphQL error: {result['errors'][0].get('message', 'Unknown error')}")
                return result
            else:
                raise Exception(f"API request failed with status {response.status_code}: {response.text}")
    
    def _encode_image_to_base64(self, image_path: str) -> str:
        """Encode image file to base64."""
        with open(image_path, 'rb') as f:
            return base64.b64encode(f.read()).decode('utf-8')
    
    async def _upload_file_to_layer(self, file_path: str) -> str:
        """Upload a file to Layer.ai and return the file URL."""
        # First, create upload URL
        mutation = """
        mutation CreateUploadUrls($input: CreateUploadUrlsInput!) {
            createUploadUrls(input: $input) {
                ... on UploadUrls {
                    uploadUrls {
                        url
                        fileId
                    }
                }
            }
        }
        """
        
        filename = os.path.basename(file_path)
        variables = {
            "input": {
                "workspaceId": self.workspace_id,
                "filenames": [filename]
            }
        }
        
        result = await self._make_graphql_request(mutation, variables)
        upload_info = result["data"]["createUploadUrls"]["uploadUrls"][0]
        upload_url = upload_info["url"]
        file_id = upload_info["fileId"]
        
        # Upload the file
        with open(file_path, 'rb') as f:
            file_content = f.read()
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            upload_response = await client.put(upload_url, content=file_content)
            if upload_response.status_code != 200:
                raise Exception(f"File upload failed: {upload_response.status_code}")
        
        # Return the file URL for Layer.ai
        return f"https://media.app.layer.ai/workspaces/{self.workspace_id}/files/{file_id}/{filename}"
    
    async def _create_asset(self, arguments: Dict[str, Any]):
        """Create a new asset using Layer.ai with comprehensive parameters."""
        prompt = arguments.get("prompt")
        if not prompt:
            return [TextContent(type="text", text="‚ùå Error: 'prompt' parameter is required")]
        
        generation_type = arguments.get("generation_type", "CREATE")
        width = arguments.get("width", 512)
        height = arguments.get("height", 512)
        quality = arguments.get("quality", "HIGH")
        steps = arguments.get("steps", 20)
        guidance_scale = arguments.get("guidance_scale", 7.5)
        negative_prompt = arguments.get("negative_prompt")
        seed = arguments.get("seed")
        creativity = arguments.get("creativity")
        resemblance = arguments.get("resemblance")
        upscale_ratio = arguments.get("upscale_ratio")
        duration_seconds = arguments.get("duration_seconds")
        transparency = arguments.get("transparency")
        tileability = arguments.get("tileability")
        include_textures = arguments.get("include_textures")
        face_limit = arguments.get("face_limit")
        input_files = arguments.get("input_files", [])
        save_path = arguments.get("save_path", "./assets/generated_asset.png")
        wait_for_completion = arguments.get("wait_for_completion", True)
        
        if isinstance(wait_for_completion, str):
            wait_for_completion = wait_for_completion.lower() == "true"
        
        # Create the asset directory if it doesn't exist
        Path(save_path).parent.mkdir(parents=True, exist_ok=True)
        
        # Build parameters object
        parameters = {
            "generationType": generation_type,
            "prompt": prompt,
            "width": width,
            "height": height,
            "quality": quality,
            "numInferenceSteps": steps,
            "guidanceScale": guidance_scale
        }
        
        # Add optional parameters
        if negative_prompt:
            parameters["negativePrompt"] = negative_prompt
        if seed:
            parameters["seed"] = seed
        if creativity is not None:
            parameters["creativity"] = creativity
        if resemblance is not None:
            parameters["resemblance"] = resemblance
        if upscale_ratio:
            parameters["upscaleRatio"] = upscale_ratio
        if duration_seconds:
            parameters["durationSeconds"] = duration_seconds
        if transparency is not None:
            parameters["transparency"] = transparency
        if tileability is not None:
            parameters["tileability"] = tileability
        if include_textures is not None:
            parameters["includeTextures"] = include_textures
        if face_limit:
            parameters["faceLimit"] = face_limit
        
        # Handle input files - upload them first
        if input_files:
            file_urls = []
            for file_path in input_files:
                if os.path.exists(file_path):
                    try:
                        file_url = await self._upload_file_to_layer(file_path)
                        file_urls.append({"url": file_url})
                    except Exception as e:
                        logger.warning(f"Failed to upload {file_path}: {e}")
            if file_urls:
                parameters["files"] = file_urls
        
        # Create the inference
        mutation = """
        mutation CreateInference($input: CreateInferenceInput!) {
            createInference(input: $input) {
                ... on Inference {
                    id
                    status
                    createdAt
                }
                ... on Error {
                    message
                }
            }
        }
        """
        
        variables = {
            "input": {
                "workspaceId": self.workspace_id,
                "parameters": parameters
            }
        }
        
        result = await self._make_graphql_request(mutation, variables)
        inference_response = result["data"]["createInference"]
        
        # Check if it's an error
        if "message" in inference_response:
            return [TextContent(type="text", text=f"‚ùå Generation failed: {inference_response['message']}")]
        
        if not inference_response or "id" not in inference_response:
            return [TextContent(type="text", text=f"‚ùå Invalid inference response: {inference_response}")]
        
        inference_id = inference_response["id"]
        
        response_text = f"""üöÄ Asset Generation Started!

üé® Prompt: {prompt}
üîß Type: {generation_type}
üìè Size: {width}x{height}
‚ö° Quality: {quality}
üÜî Inference ID: {inference_id}
üìä Status: {inference_response['status']}
üìÖ Created: {inference_response['createdAt']}

"""
        
        if not wait_for_completion:
            response_text += "‚è≥ Generation started. Use get_inference_status to check progress."
            return [TextContent(type="text", text=response_text)]
        
        # Wait for completion
        response_text += await self._wait_for_completion(inference_id, save_path)
        
        return [TextContent(type="text", text=response_text)]
    
    async def _remove_background(self, arguments: Dict[str, Any]):
        """Remove background from an image."""
        image_path = arguments.get("image_path")
        image_url = arguments.get("image_url")
        save_path = arguments.get("save_path", "./assets/no_background.png")
        wait_for_completion = arguments.get("wait_for_completion", True)
        
        if not image_path and not image_url:
            return [TextContent(type="text", text="‚ùå Error: Either 'image_path' or 'image_url' is required")]
        
        # Fixed mutation for RawImageResponse
        mutation = """
        mutation RemoveBackground($input: RemoveBackgroundInput!) {
            removeBackground(input: $input) {
                ... on RawImage {
                    uri
                    contentType
                }
                ... on Error {
                    message
                }
            }
        }
        """
        
        variables = {
            "input": {
                "workspaceId": self.workspace_id
            }
        }
        
        if image_path and os.path.exists(image_path):
            variables["input"]["imageBase64"] = self._encode_image_to_base64(image_path)
        elif image_url:
            variables["input"]["imageUrl"] = image_url
        else:
            return [TextContent(type="text", text="‚ùå Error: Image file not found")]
        
        result = await self._make_graphql_request(mutation, variables)
        bg_response = result["data"]["removeBackground"]
        
        # Check if it's an error
        if "message" in bg_response:
            return [TextContent(type="text", text=f"‚ùå Background removal failed: {bg_response['message']}")]
        
        if "uri" in bg_response:
            # Download the result from the URI
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            
            try:
                async with httpx.AsyncClient(timeout=60.0) as client:
                    download_response = await client.get(bg_response["uri"])
                    if download_response.status_code == 200:
                        with open(save_path, 'wb') as f:
                            f.write(download_response.content)
                        
                        file_size = len(download_response.content)
                        content_type = bg_response.get("contentType", "image/png")
                        
                        response_text = f"""üé® Background Removal Completed!

üìÅ File saved: {save_path}
üìä File size: {file_size:,} bytes
üéØ Content Type: {content_type}
üîó Original URL: {bg_response["uri"]}

üéÆ Ready to use!"""
                    else:
                        response_text = f"‚ùå Failed to download result: HTTP {download_response.status_code}"
            except Exception as e:
                response_text = f"‚ùå Download error: {str(e)}"
        else:
            response_text = "‚ùå No image URI received from background removal."
        
        return [TextContent(type="text", text=response_text)]
    
    async def _describe_image(self, arguments: Dict[str, Any]):
        """Get AI-generated description of an image."""
        image_path = arguments.get("image_path")
        image_url = arguments.get("image_url")
        detail_level = arguments.get("detail_level", "detailed")
        
        if not image_path and not image_url:
            return [TextContent(type="text", text="‚ùå Error: Either 'image_path' or 'image_url' is required")]
        
        # Fixed mutation for StringResponse
        mutation = """
        mutation DescribeImage($input: DescribeImageInput!) {
            describeImage(input: $input) {
                ... on StringResponse {
                    value
                }
                ... on Error {
                    message
                }
            }
        }
        """
        
        variables = {
            "input": {
                "workspaceId": self.workspace_id,
                "detailLevel": detail_level.upper()
            }
        }
        
        if image_path and os.path.exists(image_path):
            variables["input"]["imageBase64"] = self._encode_image_to_base64(image_path)
        elif image_url:
            variables["input"]["imageUrl"] = image_url
        else:
            return [TextContent(type="text", text="‚ùå Error: Image file not found")]
        
        result = await self._make_graphql_request(mutation, variables)
        desc_response = result["data"]["describeImage"]
        
        # Check if it's an error
        if "message" in desc_response:
            return [TextContent(type="text", text=f"‚ùå Image description failed: {desc_response['message']}")]
        
        description = desc_response.get("value", "No description generated")
        
        response_text = f"""üîç Image Description Generated!

üì∏ Image: {image_path or image_url}
üìä Detail Level: {detail_level}

üìù Description:
{description}

üí° You can use this description as a prompt for generating similar images!"""
        
        return [TextContent(type="text", text=response_text)]
    
    async def _generate_prompt(self, arguments: Dict[str, Any]):
        """Generate optimized prompts using Layer.ai's Prompt Genie."""
        base_prompt = arguments.get("base_prompt")
        asset_type = arguments.get("asset_type", "general")
        
        if not base_prompt:
            return [TextContent(type="text", text="‚ùå Error: 'base_prompt' parameter is required")]
        
        # Fixed mutation for StringResponse
        mutation = """
        mutation GeneratePrompt($input: GeneratePromptInput!) {
            generatePrompt(input: $input) {
                ... on StringResponse {
                    value
                }
                ... on Error {
                    message
                }
            }
        }
        """
        
        variables = {
            "input": {
                "workspaceId": self.workspace_id,
                "basePrompt": base_prompt,
                "assetType": asset_type
            }
        }
        
        result = await self._make_graphql_request(mutation, variables)
        prompt_response = result["data"]["generatePrompt"]
        
        # Check if it's an error
        if "message" in prompt_response:
            return [TextContent(type="text", text=f"‚ùå Prompt generation failed: {prompt_response['message']}")]
        
        generated_prompt = prompt_response.get("value", base_prompt)
        
        response_text = f"""‚ú® Prompt Generated!

üìù Original: {base_prompt}
üéØ Asset Type: {asset_type}

üöÄ Optimized Prompt:
{generated_prompt}

üí° You can now use this optimized prompt for better generation results!"""
        
        return [TextContent(type="text", text=response_text)]
    
    async def _wait_for_completion(self, inference_id: str, save_path: str) -> str:
        """Wait for inference completion and download result."""
        response_text = "‚è≥ Waiting for completion...\n"
        max_wait_time = 300
        check_interval = 5
        elapsed_time = 0
        
        status_query = """
        query GetInferenceStatus($input: GetInferencesByIdInput!) {
            getInferencesById(input: $input) {
                ... on InferencesResult {
                    inferences {
                        id
                        status
                        files {
                            id
                            url
                            name
                        }
                    }
                }
            }
        }
        """
        
        while elapsed_time < max_wait_time:
            status_variables = {
                "input": {
                    "inferenceIds": [inference_id]
                }
            }
            
            status_result = await self._make_graphql_request(status_query, status_variables)
            inferences = status_result["data"]["getInferencesById"]["inferences"]
            
            if not inferences:
                return "‚ùå Inference not found."
                
            inference_data = inferences[0]
            current_status = inference_data["status"]
            
            if current_status == "COMPLETE":
                response_text += f"‚úÖ Completed in {elapsed_time} seconds!\n\n"
                
                files = inference_data.get("files", [])
                if files:
                    file_info = files[0]
                    file_url = file_info["url"]
                    filename = file_info.get("name", f"result_{inference_id}")
                    
                    try:
                        async with httpx.AsyncClient(timeout=60.0) as client:
                            download_response = await client.get(file_url)
                            if download_response.status_code == 200:
                                # Determine file extension from content type
                                content_type = download_response.headers.get('content-type', '')
                                if 'image' in content_type:
                                    if 'png' in content_type:
                                        ext = '.png'
                                    elif 'jpeg' in content_type or 'jpg' in content_type:
                                        ext = '.jpg'
                                    elif 'gif' in content_type:
                                        ext = '.gif'
                                    elif 'svg' in content_type:
                                        ext = '.svg'
                                    else:
                                        ext = '.png'
                                elif 'video' in content_type:
                                    ext = '.mp4'
                                elif 'audio' in content_type:
                                    ext = '.wav'
                                elif 'model' in content_type or 'application/octet-stream' in content_type:
                                    ext = '.glb'
                                else:
                                    ext = Path(filename).suffix or '.png'
                                
                                # Fix file path logic
                                if save_path.endswith('/'):
                                    full_path = Path(save_path) / (filename + ext)
                                else:
                                    full_path = Path(save_path)
                                    if full_path.suffix == '':
                                        full_path = full_path.with_suffix(ext)
                                
                                Path(full_path).parent.mkdir(parents=True, exist_ok=True)
                                
                                with open(full_path, 'wb') as f:
                                    f.write(download_response.content)
                                
                                file_size = len(download_response.content)
                                
                                response_text += f"""üìÅ File saved: {full_path}
üìä File size: {file_size:,} bytes
üîó Original URL: {file_url}
üéØ Content Type: {content_type}

üéÆ Ready to use!"""
                            else:
                                response_text += f"‚ùå Download failed: HTTP {download_response.status_code}"
                    except Exception as e:
                        response_text += f"‚ùå Download error: {str(e)}"
                else:
                    response_text += "‚ö†Ô∏è No files generated."
                
                break
                
            elif current_status == "FAILED":
                response_text += f"‚ùå Failed after {elapsed_time} seconds."
                break
                
            elif current_status == "CANCELLED":
                response_text += f"‚ö†Ô∏è Cancelled after {elapsed_time} seconds."
                break
            
            await asyncio.sleep(check_interval)
            elapsed_time += check_interval
        
        if elapsed_time >= max_wait_time:
            response_text += f"‚è∞ Timed out after {max_wait_time} seconds."
        
        return response_text
    
    async def _get_workspace_info(self, arguments: Dict[str, Any]):
        """Get workspace information."""
        response_text = f"""üìä Workspace Information

üè¢ Workspace ID: {self.workspace_id}
üîë API Token: [CONFIGURED]

‚úÖ Layer.ai Comprehensive MCP Server is connected and ready!

üé® Available Generation Types:
‚Ä¢ CREATE - Generate new assets from text
‚Ä¢ IMAGE_TO_3D - Convert images to 3D models  
‚Ä¢ TEXT_TO_3D - Generate 3D models from text
‚Ä¢ VECTORIZE_IMAGE - Convert to vector format
‚Ä¢ REMOVE_BACKGROUND - Remove image backgrounds
‚Ä¢ UPSCALE - Increase image resolution
‚Ä¢ IMAGE_TO_VIDEO - Convert images to video
‚Ä¢ TEXT_TO_SPEECH - Generate speech from text
‚Ä¢ SOUND_EFFECT - Create audio effects
‚Ä¢ And many more!

üõ†Ô∏è Fixed Features:
‚úÖ Proper GraphQL response handling
‚úÖ File upload for 3D/vector generation
‚úÖ Background removal with direct output
‚úÖ Image description with StringResponse
‚úÖ Prompt generation with StringResponse"""
        
        return [TextContent(type="text", text=response_text)]
    
    async def run(self):
        """Run the MCP server."""
        async with stdio_server() as (read_stream, write_stream):
            await self.server.run(
                read_stream,
                write_stream,
                InitializationOptions(
                    server_name="layer-ai-comprehensive",
                    server_version="3.0.0",
                    capabilities=self.server.get_capabilities(
                        notification_options=NotificationOptions(),
                        experimental_capabilities={}
                    )
                )
            )


async def main():
    """Main entry point."""
    logger.info("üöÄ Starting Fully Working Comprehensive Layer.ai MCP Server...")
    server = FullyWorkingLayerMCPServer()
    await server.run()


if __name__ == "__main__":
    asyncio.run(main())